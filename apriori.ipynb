{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "from scipy.sparse import csr_matrix\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras transacciones:\n",
      "[['Organic Celery Hearts', 'Organic 4% Milk Fat Whole Milk Cottage Cheese', 'Bag of Organic Bananas', 'Organic Whole String Cheese', 'Lightly Smoked Sardines in Olive Oil', 'Organic Hass Avocado', 'Bulgarian Yogurt', 'Cucumber Kirby'], ['Spring Water', 'Prosciutto, Americano', 'Grated Pecorino Romano Cheese', 'Super Greens Salad', 'Cage Free Extra Large Grade AA Eggs', 'Asparagus', 'Organic Garnet Sweet Potato (Yam)', 'Organic Half & Half'], ['Organic Raw Unfiltered Apple Cider Vinegar', 'Shelled Pistachios', 'Organic Biologique Limes', 'Organic Baby Arugula', 'Organic Hot House Tomato', 'Bunched Cilantro', 'Green Peas', 'Fresh Dill', 'Flat Parsley, Bunch'], ['Roasted Turkey', 'Organic Whole Strawberries', 'Organic Pomegranate Kernels', 'Organic Raspberries', 'Organic Cucumber', 'Organic Blueberries', 'Organic Grape Tomatoes'], ['Organic Whole Grassmilk Milk', 'Garbanzo Beans', 'Geranium Liquid Dish Soap', 'Corn Maize Tortillas', 'Organic Chocolate Almondmilk Pudding', 'Pinto Beans No Salt Added', 'Natural Spring Water', 'Organic 2% Buttermilk', 'Uncured Applewood Smoked Bacon', 'Bag of Organic Bananas', 'Organic Extra Virgin Oil Olive', 'Organic Cinnamon Apple Sauce', 'Plastic Wrap', '100% Organic Unbleached All-Purpose Flour', 'Organic Ketchup', 'Organic Orange Juice With Calcium & Vitamin D', 'Mild Diced Green Chiles', 'Organic Yellow Onion', 'Organic Garlic', 'Organic Coconut Milk', 'Organic Lemonade', 'Uncured Genoa Salami', 'Organic Seasoned Yukon Select Potatoes Hashed Browns', 'Black Beans No Salt Added', 'Organic Raspberries', 'Organic Raw Kombucha Gingerade', 'Aluminum Foil', 'Olive Oil & Aloe Vera Hand Soap', 'Tomatoes, Crushed, Organic', 'Organic Italian Parsley Bunch', 'Black Beans', 'Organic Unsweetened Almond Milk', 'Organic Corn Starch', 'Organic Hothouse Cucumbers', 'Organic Sliced Provalone Cheese', 'Guacamole', 'Raspberry Sorbet Pops', 'Plastic Spoons', 'Crackers, Oyster', 'Whole Milk Greek Blended Vanilla Bean Yogurt', 'Organic Stringles Mozzarella String Cheese', 'Organic Zucchini', 'Sliced Pepperoni', 'Baby Swiss Slices Cheese', 'Lavender Scent Laundry Detergent', 'Organic Free Range Chicken Broth', 'Queso Fresco', 'Unsalted Cultured Butter', 'Natural Chicken & Maple Breakfast Sausage Patty']]\n"
     ]
    }
   ],
   "source": [
    "products = pd.read_csv('order_products__train.csv')\n",
    "orders = pd.read_csv('products.csv')\n",
    "\n",
    "dfMerged = pd.merge(orders, products, on=\"product_id\", how=\"inner\")\n",
    "\n",
    "# Agrupar productos por transacción para crear listas de transacciones\n",
    "\n",
    "transactions = dfMerged.groupby(\"order_id\")[\"product_name\"].apply(list).tolist()\n",
    "print(f\"Primeras transacciones:\\n{transactions[:5]}\")\n",
    "\n",
    "item_mapping = {item: idx for idx, item in enumerate(sorted(set(item for transaction in transactions for item in transaction)))}\n",
    "rows, cols = [], []\n",
    "for row_idx, transaction in enumerate(transactions):\n",
    "    for item in transaction:\n",
    "        rows.append(row_idx)\n",
    "        cols.append(item_mapping[item])\n",
    "\n",
    "# Crear la matriz dispersa\n",
    "order_matrix_sparse = csr_matrix(([1] * len(rows), (rows, cols)), shape=(len(transactions), len(item_mapping)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular soporte de elementos individuales\n",
    "#def calculate_support(item_indices, data_matrix):\n",
    "    \n",
    "    #item_mask = data_matrix[:, item_indices].toarray().all(axis=1)\n",
    "    #support = np.sum(item_mask) / data_matrix.shape[0]\n",
    "    #return support\n",
    "\n",
    "#!DESCOMENTAR EN CASO DE NO FUNCIONARLA FUNCION DE ARRIBA (KEVIN A.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mejora de funcion para calcular el soporte V1.2\n",
    "def calculate_support(item_indices, data_matrix):\n",
    "    #para un solo indice, optimizamos el calculo\n",
    "    if isinstance(item_indices, int) or (hasattr(item_indices, '__len__') and len((item_indices) == 1)):\n",
    "        if hasattr(item_indices, '__len__'):\n",
    "            idx = item_indices[0]\n",
    "        else:\n",
    "            idx = item_indices\n",
    "            #Contamos las ocurrencias de la columna \n",
    "        col_sum = data_matrix.getcol(idx).sum()\n",
    "        return float(col_sum) / data_matrix.shape[0]\n",
    "    \n",
    "    #Ahora para multiples indices, operamos conjuntos en las filas \n",
    "    row_with_items = set(range(data_matrix.shape[0]))\n",
    "    for idx in  item_indices:\n",
    "        #Obtenemos los indices de filas donde aparece este item\n",
    "        col = data_matrix.getcol(idx)\n",
    "        rows_with_time = set(col.nonzero()[0])\n",
    "        #interseccion con filas que contienen todos los items anteriores \n",
    "        row_with_items &= rows_with_item\n",
    "    \n",
    "    support = len(rows_with_items) / data_matrix.shape[0]\n",
    "    return support "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para generar ítems frecuentes utilizando matrices dispersas\n",
    "def apriori_manual(data_matrix, min_support):\n",
    "  \n",
    "    num_items = data_matrix.shape[1]\n",
    "    frequent_itemsets = []\n",
    "    current_itemsets = [[i] for i in range(num_items)]\n",
    "    \n",
    "    while current_itemsets:\n",
    "        next_itemsets = []\n",
    "        item_supports = []\n",
    "        \n",
    "        # Calcular soporte para cada conjunto actual\n",
    "        for itemset in current_itemsets:\n",
    "            support = calculate_support(itemset, data_matrix)\n",
    "            if support >= min_support:\n",
    "                frequent_itemsets.append((itemset, support))\n",
    "                item_supports.append(itemset)\n",
    "        \n",
    "        # Generar nuevas combinaciones de ítems frecuentes actuales\n",
    "        for i in range(len(item_supports)):\n",
    "            for j in range(i + 1, len(item_supports)):\n",
    "                combined_itemset = sorted(set(item_supports[i]) | set(item_supports[j]))\n",
    "                if len(combined_itemset) == len(item_supports[i]) + 1:\n",
    "                    next_itemsets.append(combined_itemset)\n",
    "        \n",
    "        current_itemsets = next_itemsets  # Actualizar conjuntos actuales\n",
    "    \n",
    "    return frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertimos la matriz dispersa a una lista de listas \n",
    "def sparse_transaction(data_matrix, item_mapping):\n",
    "    reverse_mapping = {idx: item for item, idx in item_mapping.items()}\n",
    "\n",
    "    transactions = []\n",
    "    #Aplicamos la función a cada fila de la matriz dispersa\n",
    "    for i in range(data_matrix.shape[0]):\n",
    "        #obtener indices de elementos no cero\n",
    "        row = data_matrix[i]\n",
    "        row_indices = row.indices\n",
    "        \n",
    "        # Convertir índices a nombres de productos usando el mapeo inverso\n",
    "        transaction = []\n",
    "        for idx in row_indices:\n",
    "            if idx in reverse_mapping:\n",
    "                transaction.append(reverse_mapping[idx])\n",
    "            else:\n",
    "                print(f\"Advertencia: Índice {idx} no encontrado en el mapeo inverso\")\n",
    "        \n",
    "        transactions.append(transaction)\n",
    "    \n",
    "    return transactions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primera version (Esta funcion no es muy optima)\n",
    "# def generar_candidatos(itemsets_frecuentes_k, k,max_candidates= 100000):\n",
    "    # if len(itemsets_frecuentes_k) > 1000:\n",
    "    #     print(f\"Demasiados itemsets frecuentes ({len(itemsets_frecuentes_k)}). Limitando a los 1000 más frecuentes.\")\n",
    "    #     # Aquí asumimos que los itemsets ya están ordenados por soporte\n",
    "    #     itemsets_frecuentes_k = itemsets_frecuentes_k[:1000]\n",
    "    # candidatos = []\n",
    "    # n = len(itemsets_frecuentes_k)\n",
    "    \n",
    "    # join => combinar itemsets que comparten k-1 elementos\n",
    "    # for i in range(n):\n",
    "    #     for j in range(i+1, n):\n",
    "    #         # k=1 simplemente combinamos los items\n",
    "    #         if k == 1:\n",
    "    #             candidato = [itemsets_frecuentes_k[i][0], itemsets_frecuentes_k[j][0]]\n",
    "    #             candidatos.append(sorted(candidato))\n",
    "    #         else:\n",
    "    #             # Para k>1 verificamos si los primeros k-1  son iguales\n",
    "    #             if itemsets_frecuentes_k[i][:k-1] == itemsets_frecuentes_k[j][:k-1]:\n",
    "    #                 # Combinamos\n",
    "    #                 candidato = itemsets_frecuentes_k[i].copy()\n",
    "    #                 candidato.append(itemsets_frecuentes_k[j][k-1])\n",
    "    #                 candidato.sort()\n",
    "    #                 candidatos.append(candidato)\n",
    "    \n",
    "    # # Eliminar duplicados usando un conjunto (set)\n",
    "    # candidatos_unicos = [list(x) for x in set(tuple(x) for x in candidatos)]\n",
    "    # return candidatos_unicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Version 1.2\n",
    "def generar_candidatos(itemsets_frecuentes_k, k, max_candidates=100000, support_dict=None):\n",
    "    # Limitar itemsets frecuentes si hay demasiados\n",
    "    if len(itemsets_frecuentes_k) > 1000:\n",
    "        print(f\"Demasiados itemsets frecuentes ({len(itemsets_frecuentes_k)}). Limitando a los 1000 más frecuentes.\")\n",
    "        if support_dict:\n",
    "            # Ordenar por soporte si tenemos el diccionario de soporte\n",
    "            itemsets_frecuentes_k = sorted(itemsets_frecuentes_k, \n",
    "                                          key=lambda x: support_dict.get(tuple(x), 0), \n",
    "                                          reverse=True)[:1000]\n",
    "        else:\n",
    "            itemsets_frecuentes_k = itemsets_frecuentes_k[:1000]\n",
    "    \n",
    "    # Estimación del número de candidatos que se generarán\n",
    "    n = len(itemsets_frecuentes_k)\n",
    "    estimated_candidates = n * (n - 1) // 2\n",
    "    \n",
    "    # Si la estimación supera el máximo, aplicamos filtrado adicional\n",
    "    if estimated_candidates > max_candidates:\n",
    "        reduction_factor = max_candidates / estimated_candidates\n",
    "        limit = int(n * reduction_factor**0.5)\n",
    "        print(f\"Estimación de candidatos ({estimated_candidates}) excede el máximo. Limitando a {limit} itemsets.\")\n",
    "        if support_dict:\n",
    "            itemsets_frecuentes_k = sorted(itemsets_frecuentes_k, \n",
    "                                          key=lambda x: support_dict.get(tuple(x), 0), \n",
    "                                          reverse=True)[:limit]\n",
    "        else:\n",
    "            itemsets_frecuentes_k = itemsets_frecuentes_k[:limit]\n",
    "        n = len(itemsets_frecuentes_k)\n",
    "    \n",
    "    # Usar un conjunto para evitar duplicados desde el principio\n",
    "    candidatos_set = set()\n",
    "    \n",
    "    # Para k=1, usar un enfoque más eficiente\n",
    "    if k == 1:\n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                candidato = tuple(sorted([itemsets_frecuentes_k[i][0], itemsets_frecuentes_k[j][0]]))\n",
    "                candidatos_set.add(candidato)\n",
    "    else:\n",
    "        # Optimización: creo un índice para agrupar itemsets que comparten prefijo\n",
    "        prefix_index = {}\n",
    "        for idx, itemset in enumerate(itemsets_frecuentes_k):\n",
    "            prefix = tuple(itemset[:k-1])\n",
    "            if prefix not in prefix_index:\n",
    "                prefix_index[prefix] = []\n",
    "            prefix_index[prefix].append(idx)\n",
    "        \n",
    "        # Generar candidatos solo entre itemsets con el mismo prefijo\n",
    "        for prefix, indices in prefix_index.items():\n",
    "            if len(indices) > 1:  # Necesitamos al menos 2 itemsets con el mismo prefijo\n",
    "                for i in range(len(indices)):\n",
    "                    for j in range(i+1, len(indices)):\n",
    "                        idx1, idx2 = indices[i], indices[j]\n",
    "                        # Crear candidato\n",
    "                        candidato = list(prefix) + [itemsets_frecuentes_k[idx1][k-1], itemsets_frecuentes_k[idx2][k-1]]\n",
    "                        candidato.sort()\n",
    "                        candidatos_set.add(tuple(candidato))\n",
    "    \n",
    "    # Convertir de vuelta a lista de listas\n",
    "    candidatos_unicos = [list(x) for x in candidatos_set]\n",
    "    \n",
    "    # Verificar si aún tenemos demasiados candidatos\n",
    "    if len(candidatos_unicos) > max_candidates:\n",
    "        print(f\"Aún hay demasiados candidatos ({len(candidatos_unicos)}). Limitando a {max_candidates}.\")\n",
    "        candidatos_unicos = candidatos_unicos[:max_candidates]\n",
    "    \n",
    "    return candidatos_unicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poda_apriori(candidatos, itemsets_frecuentes_k, k):\n",
    "    # Convertir itemsets frecuentes a conjunto para búsqueda eficiente O\n",
    "    itemsets_frecuentes_set = set(tuple(x) for x in itemsets_frecuentes_k)\n",
    "    candidatos_podados = []\n",
    "    \n",
    "    for candidato in candidatos:\n",
    "        es_valido = True\n",
    "        \n",
    "        # Generar todos los subconjuntos de tamaño k \n",
    "        for i in range(len(candidato)):\n",
    "            subconjunto = candidato.copy()\n",
    "            subconjunto.pop(i)\n",
    "            \n",
    "            # si algun subconunto no es frecuente , el candidato no puede ser frecuente\n",
    "            if tuple(subconjunto) not in itemsets_frecuentes_set:\n",
    "                es_valido = False\n",
    "                break\n",
    "        \n",
    "        if es_valido:\n",
    "            candidatos_podados.append(candidato)\n",
    "    \n",
    "    return candidatos_podados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementacion manual dde priori con procesamiento por lotes\n",
    "def apriori_lotes(transactions_list, min_support, batch_size=50000):\n",
    "    n_total = len(transactions_list)\n",
    "    print(f\"Procesando {n_total} transacciones con soporte minimo {min_support}\")\n",
    "\n",
    "    item_counts = {}\n",
    "\n",
    "    n_batches = (n_total + batch_size - 1) // batch_size\n",
    "\n",
    "    print('Fase 1: Generando 1-itemsets frecuentes')\n",
    "\n",
    "    #contar items individuales\n",
    "    for i in range(n_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, n_total)\n",
    "        print(f\"Procesando lote {i + 1}/ {n_batches} (transacciones {start_idx}-{end_idx})\")\n",
    "        \n",
    "        # Transacciones para este lote\n",
    "        batch_transactions = transactions_list[start_idx:end_idx]\n",
    "\n",
    "        # Contador de items individuales\n",
    "        for transaction in batch_transactions:\n",
    "            for item in transaction:\n",
    "                item_counts[item] = item_counts.get(item, 0) + 1\n",
    "\n",
    "    # Filtrado por soporte mínimo\n",
    "    frequent_1_itemsets = []\n",
    "    support_dict = {}\n",
    "\n",
    "    for item, count in item_counts.items():\n",
    "        support = count / n_total\n",
    "        if support >= min_support:\n",
    "            frequent_1_itemsets.append([item])\n",
    "            support_dict[tuple([item])] = support\n",
    "    \n",
    "    print(f\"  Se encontraron {len(frequent_1_itemsets)} 1-itemsets frecuentes\")\n",
    "\n",
    "    all_frequent_itemsets = {1: frequent_1_itemsets}\n",
    "\n",
    "    k = 1\n",
    "    while k in all_frequent_itemsets and all_frequent_itemsets[k]:  # Verificación más explícita\n",
    "        print(f\"Fase {k + 1}: Generando {k + 1}-itemsets frecuentes\")\n",
    "\n",
    "        if len(all_frequent_itemsets[k]) > 1000:\n",
    "            print(f\"Demasiados itemsets frecuentes ({len(all_frequent_itemsets[k])}). Limitando a los 1000 con mayor soporte.\")\n",
    "           \n",
    "            sorted_itemsets = sorted(all_frequent_itemsets[k], \n",
    "                                    key=lambda x: support_dict[tuple(x)], \n",
    "                                    reverse=True)[:1000]\n",
    "            all_frequent_itemsets[k] = sorted_itemsets\n",
    "            print(f\"Limitado a {len(all_frequent_itemsets[k])} itemsets frecuentes\")\n",
    "        \n",
    "        # Generación\n",
    "        candidatos = generar_candidatos(all_frequent_itemsets[k], k)\n",
    "\n",
    "        if len(candidatos) > 100000:\n",
    "            print(f\"¡Demasiados candidatos ({len(candidatos)})! Aumentando el umbral de soporte para esta fase.\")\n",
    "            # Aumentar temporalmente el umbral de soporte\n",
    "            temp_min_support = min_support * 2\n",
    "            print(f\"Umbral de soporte temporal: {temp_min_support}\")\n",
    "\n",
    "            # Filtrar itemsets frecuentes con el nuevo umbral\n",
    "            filtered_itemsets = []\n",
    "            for itemset in all_frequent_itemsets[k]:\n",
    "                if support_dict[tuple(itemset)] >= temp_min_support:\n",
    "                    filtered_itemsets.append(itemset)\n",
    "            \n",
    "            print(f\"Filtrando a {len(filtered_itemsets)} itemsets con mayor soporte\")\n",
    "            # Regenerar candidatos con el conjunto filtrado\n",
    "            candidatos = generar_candidatos(filtered_itemsets, k)\n",
    "            print(f\"Nuevos candidatos generados: {len(candidatos)}\")\n",
    "    \n",
    "        # Aplicación de poda\n",
    "        candidatos_podados = poda_apriori(candidatos, all_frequent_itemsets[k], k)\n",
    "\n",
    "        print(f\"Generados {len(candidatos)} candidatos, {len(candidatos_podados)} despues de poda\")\n",
    "\n",
    "        if not candidatos_podados:\n",
    "            print(f\"No hay candidatos después de la poda. Terminando.\")\n",
    "            break\n",
    "\n",
    "        # Dic para apariciones de candidatos\n",
    "        candidato_counts = {tuple(c): 0 for c in candidatos_podados}\n",
    "    \n",
    "        # Procesado por lotes para contar candidatos \n",
    "        for i in range(n_batches): \n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min((i + 1) * batch_size, n_total)\n",
    "\n",
    "            print(f\"Procesando lote {i+1}/{n_batches} (transacciones {start_idx}-{end_idx})\")\n",
    "\n",
    "            batch_transactions = transactions_list[start_idx:end_idx]\n",
    "            \n",
    "            # Precomputar los conjuntos de transacciones\n",
    "            transaction_sets = [set(transaction) for transaction in batch_transactions]\n",
    "            \n",
    "            # Usar un enfoque más eficiente para verificar candidatos\n",
    "            for t_idx, transaction_set in enumerate(transaction_sets):\n",
    "                # Solo procesar transacciones que tengan suficientes elementos\n",
    "                if len(transaction_set) >= k + 1:\n",
    "                    for candidato in candidatos_podados:\n",
    "                        # Verificación más rápida usando issubset\n",
    "                        if set(candidato).issubset(transaction_set):\n",
    "                            candidato_counts[tuple(candidato)] += 1\n",
    "                \n",
    "                # Mostrar progreso \n",
    "                if (t_idx + 1) % 10000 == 0:\n",
    "                    print(f\"  Progreso: {t_idx + 1}/{len(batch_transactions)} transacciones procesadas\")\n",
    "        \n",
    "        frequent_itemsets_k_plus_1 = []\n",
    "        \n",
    "        for candidato, count in candidato_counts.items():\n",
    "            support = count / n_total\n",
    "            if support >= min_support:\n",
    "                frequent_itemsets_k_plus_1.append(list(candidato))\n",
    "                support_dict[candidato] = support\n",
    "        \n",
    "        print(f\"  Se encontraron {len(frequent_itemsets_k_plus_1)} {k+1}-itemsets frecuentes\")\n",
    "        \n",
    "        # Continua si encuentra itemsets frecuentes\n",
    "        if frequent_itemsets_k_plus_1:\n",
    "            all_frequent_itemsets[k+1] = frequent_itemsets_k_plus_1\n",
    "            k += 1  # Incrementar k solo después de procesar todos los lotes\n",
    "        else:\n",
    "            print(f\"No se encontraron {k+1}-itemsets frecuentes. Terminando.\")\n",
    "            break\n",
    "            \n",
    "    return all_frequent_itemsets, support_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para generar reglas\n",
    "def generar_reglas(itemsets_frecuentes, support_dict, min_confidence=0.5):\n",
    "    rules = []\n",
    "\n",
    "    for k in range(2, len(itemsets_frecuentes) + 1):\n",
    "        if k not in itemsets_frecuentes:\n",
    "            continue\n",
    "        for itemset in itemsets_frecuentes[k]:\n",
    "            itemset_support = support_dict[tuple(itemset)]\n",
    "            for i in range(len(itemset)):\n",
    "                from itertools import combinations\n",
    "                for antecedent_items in combinations(itemset, i):\n",
    "                    antecedent = list(antecedent_items)\n",
    "                    consequent = [item for item in itemset if item not in antecedent]\n",
    "                    \n",
    "                    # soporte antecedente\n",
    "                    antecedent_support = support_dict[tuple(antecedent)]\n",
    "                    \n",
    "                    # Confianzita\n",
    "                    confidence = itemset_support / antecedent_support\n",
    "                    \n",
    "                    # Si cumple con la confianza mínima, se agregar\n",
    "                    if confidence >= min_confidence:\n",
    "                        # lift\n",
    "                        consequent_support = support_dict[tuple(consequent)]\n",
    "                        lift = confidence / consequent_support\n",
    "                        \n",
    "                        rules.append({\n",
    "                            'antecedent': antecedent,\n",
    "                            'consequent': consequent,\n",
    "                            'support': itemset_support,\n",
    "                            'confidence': confidence,\n",
    "                            'lift': lift\n",
    "                        })\n",
    "    \n",
    "    \n",
    "    if rules:\n",
    "        rulesDf = pd.DataFrame(rules)\n",
    "\n",
    "        #lift descendente\n",
    "        rulesDf = rulesDf.sort_values('lift', ascending=False).reset_index(drop=True)\n",
    "        return rulesDf\n",
    "    else:\n",
    "        return pd.DataFrame(columns=['antecedent', 'consequent', 'support', 'confidence', 'lift'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de transacciones en transactions_list: 131209\n",
      "Primeras 3 transacciones:\n",
      "Transacción 0 (longitud 8): ['Bag of Organic Bananas', 'Bulgarian Yogurt', 'Cucumber Kirby', 'Lightly Smoked Sardines in Olive Oil', 'Organic 4% Milk Fat Whole Milk Cottage Cheese']\n",
      "Transacción 1 (longitud 8): ['Asparagus', 'Cage Free Extra Large Grade AA Eggs', 'Grated Pecorino Romano Cheese', 'Organic Garnet Sweet Potato (Yam)', 'Organic Half & Half']\n",
      "Transacción 2 (longitud 9): ['Bunched Cilantro', 'Flat Parsley, Bunch', 'Fresh Dill', 'Green Peas', 'Organic Baby Arugula']\n",
      "Transacciones vacías: 0 de 131209\n",
      "Top 10 items más frecuentes:\n",
      "Item: Banana, Frecuencia: 18726, Soporte: 0.142719\n",
      "Item: Bag of Organic Bananas, Frecuencia: 15480, Soporte: 0.117980\n",
      "Item: Organic Strawberries, Frecuencia: 10894, Soporte: 0.083028\n",
      "Item: Organic Baby Spinach, Frecuencia: 9784, Soporte: 0.074568\n",
      "Item: Large Lemon, Frecuencia: 8135, Soporte: 0.062000\n",
      "Item: Organic Avocado, Frecuencia: 7409, Soporte: 0.056467\n",
      "Item: Organic Hass Avocado, Frecuencia: 7293, Soporte: 0.055583\n",
      "Item: Strawberries, Frecuencia: 6494, Soporte: 0.049494\n",
      "Item: Limes, Frecuencia: 6033, Soporte: 0.045980\n",
      "Item: Organic Raspberries, Frecuencia: 5546, Soporte: 0.042268\n"
     ]
    }
   ],
   "source": [
    "#aplicacion en todo el algoritmo\n",
    "transactions_list = sparse_transaction(order_matrix_sparse, item_mapping)\n",
    "\n",
    "# Verificar el resultado\n",
    "print(f\"Número de transacciones en transactions_list: {len(transactions_list)}\")\n",
    "if len(transactions_list) > 0:\n",
    "    print(\"Primeras 3 transacciones:\")\n",
    "    for i in range(min(3, len(transactions_list))):\n",
    "        print(f\"Transacción {i} (longitud {len(transactions_list[i])}): {transactions_list[i][:5]}\")\n",
    "\n",
    "#Verificacion vacias\n",
    "empty_transactions = sum(1 for t in transactions_list if len(t) == 0)\n",
    "print(f\"Transacciones vacías: {empty_transactions} de {len(transactions_list)}\")\n",
    "\n",
    "#distribucion frecuencias items\n",
    "item_freq = {}\n",
    "for transaction in transactions_list:\n",
    "    for item in transaction:\n",
    "        item_freq[item] = item_freq.get(item, 0) + 1\n",
    "\n",
    "#organizar por frecuencia\n",
    "sorted_items = sorted(item_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#y vreificamos cuantos superarin diferentes umbrales\n",
    "print(\"Top 10 items más frecuentes:\")\n",
    "for item, count in sorted_items[:10]:\n",
    "    support = count / len(transactions_list)\n",
    "    print(f\"Item: {item}, Frecuencia: {count}, Soporte: {support:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de transacciones en transactions_list: 131209\n",
      "Muestra de transacciones:\n",
      "Transacción 0: ['Bag of Organic Bananas', 'Bulgarian Yogurt', 'Cucumber Kirby', 'Lightly Smoked Sardines in Olive Oil', 'Organic 4% Milk Fat Whole Milk Cottage Cheese']...\n",
      "Transacción 1: ['Asparagus', 'Cage Free Extra Large Grade AA Eggs', 'Grated Pecorino Romano Cheese', 'Organic Garnet Sweet Potato (Yam)', 'Organic Half & Half']...\n",
      "Transacción 2: ['Bunched Cilantro', 'Flat Parsley, Bunch', 'Fresh Dill', 'Green Peas', 'Organic Baby Arugula']...\n",
      "Transacción 3: ['Organic Blueberries', 'Organic Cucumber', 'Organic Grape Tomatoes', 'Organic Pomegranate Kernels', 'Organic Raspberries']...\n",
      "Transacción 4: ['100% Organic Unbleached All-Purpose Flour', 'Aluminum Foil', 'Baby Swiss Slices Cheese', 'Bag of Organic Bananas', 'Black Beans']...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Número de transacciones en transactions_list: {len(transactions_list)}\")\n",
    "print(\"Muestra de transacciones:\")\n",
    "for i in range(min(5, len(transactions_list))):\n",
    "    print(f\"Transacción {i}: {transactions_list[i][:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items con soporte >= 0.1: 2\n",
      "Items con soporte >= 0.05: 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items con soporte >= 0.01: 104\n",
      "Items con soporte >= 0.005: 256\n",
      "Procesando 131209 transacciones con soporte minimo 0.01\n",
      "Fase 1: Generando 1-itemsets frecuentes\n",
      "Procesando lote 1/ 2 (transacciones 0-100000)\n",
      "Procesando lote 2/ 2 (transacciones 100000-131209)\n",
      "  Se encontraron 104 1-itemsets frecuentes\n",
      "Fase 2: Generando 2-itemsets frecuentes\n",
      "Generados 5356 candidatos, 5356 despues de poda\n",
      "Procesando lote 1/2 (transacciones 0-100000)\n",
      "  Progreso: 10000/100000 transacciones procesadas\n",
      "  Progreso: 20000/100000 transacciones procesadas\n",
      "  Progreso: 30000/100000 transacciones procesadas\n",
      "  Progreso: 40000/100000 transacciones procesadas\n",
      "  Progreso: 50000/100000 transacciones procesadas\n",
      "  Progreso: 60000/100000 transacciones procesadas\n",
      "  Progreso: 70000/100000 transacciones procesadas\n",
      "  Progreso: 80000/100000 transacciones procesadas\n",
      "  Progreso: 90000/100000 transacciones procesadas\n",
      "  Progreso: 100000/100000 transacciones procesadas\n",
      "Procesando lote 2/2 (transacciones 100000-131209)\n",
      "  Progreso: 10000/31209 transacciones procesadas\n",
      "  Progreso: 20000/31209 transacciones procesadas\n",
      "  Progreso: 30000/31209 transacciones procesadas\n",
      "  Se encontraron 16 2-itemsets frecuentes\n",
      "Fase 3: Generando 3-itemsets frecuentes\n",
      "Generados 22 candidatos, 7 despues de poda\n",
      "Procesando lote 1/2 (transacciones 0-100000)\n",
      "  Progreso: 10000/100000 transacciones procesadas\n",
      "  Progreso: 20000/100000 transacciones procesadas\n",
      "  Progreso: 30000/100000 transacciones procesadas\n",
      "  Progreso: 40000/100000 transacciones procesadas\n",
      "  Progreso: 50000/100000 transacciones procesadas\n",
      "  Progreso: 60000/100000 transacciones procesadas\n",
      "  Progreso: 70000/100000 transacciones procesadas\n",
      "  Progreso: 80000/100000 transacciones procesadas\n",
      "  Progreso: 90000/100000 transacciones procesadas\n",
      "  Progreso: 100000/100000 transacciones procesadas\n",
      "Procesando lote 2/2 (transacciones 100000-131209)\n",
      "  Progreso: 10000/31209 transacciones procesadas\n",
      "  Progreso: 20000/31209 transacciones procesadas\n",
      "  Progreso: 30000/31209 transacciones procesadas\n",
      "  Se encontraron 0 3-itemsets frecuentes\n",
      "No se encontraron 3-itemsets frecuentes. Terminando.\n"
     ]
    }
   ],
   "source": [
    "# verificacion para observar distintos umbrales\n",
    "thresholds = [0.1, 0.05, 0.01, 0.005]\n",
    "for threshold in thresholds:\n",
    "    items_above = sum(1 for _, count in item_freq.items() if count/len(transactions_list) >= threshold)\n",
    "    print(f\"Items con soporte >= {threshold}: {items_above}\")\n",
    "\n",
    "min_support = 0.01  # Reducir el umbral de soporte en caso de(se redujo al minimo posible para ver si funciona)\n",
    "\n",
    "start_time = time.time()\n",
    "frequent_itemsets, support_dict = apriori_lotes(\n",
    "    transactions_list, \n",
    "    min_support=min_support,\n",
    "    batch_size=100000\n",
    ")\n",
    "apriori_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resumen de itemsets frecuentes:\n",
      "  1-itemsets: 104\n",
      "  2-itemsets: 16\n"
     ]
    }
   ],
   "source": [
    "#itemsets frecuentes (resumen)\n",
    "print(\"\\nResumen de itemsets frecuentes:\")\n",
    "for k, itemsets in frequent_itemsets.items():\n",
    "    print(f\"  {k}-itemsets: {len(itemsets)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
