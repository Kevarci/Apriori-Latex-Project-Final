{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VERSION DE CÓDIGO 1 (fue el mostrado en la llamada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar la biblioteca necesaria para el manejo de datos\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar los datos desde URLs públicas (asegúrate de que estén disponibles)\n",
    "url_products = \"https://github.com/it-ces/Rules-puj/blob/main/products.csv?raw=true\"  # Archivo de productos\n",
    "url_orders = \"https://github.com/it-ces/Rules-puj/blob/main/order_products__train.csv?raw=true\"  # Archivo de pedidos\n",
    "\n",
    "products = pd.read_csv(url_products)  # Leer los datos de productos\n",
    "orders = pd.read_csv(url_orders)  # Leer los datos de pedidos\n",
    "\n",
    "# Merge (unión) de las tablas usando el identificador de producto para relacionarlas\n",
    "dfMerged = pd.merge(orders, products, on=\"product_id\", how=\"inner\")\n",
    "# Esto genera un DataFrame combinado que relaciona los pedidos con los nombres de los productos\n",
    "\n",
    "# Agrupar los productos por ID de pedido para formar transacciones (listas de productos comprados juntos)\n",
    "transactions = dfMerged.groupby(\"order_id\")[\"product_name\"].apply(list).tolist()\n",
    "print(f\"Primeras transacciones:\\n{transactions[:5]}\")  # Mostrar un ejemplo de las primeras transacciones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comentario del bloque:\n",
    "Propósito: Este bloque prepara los datos para la ejecución del Apriori. Los datos sin procesar se convierten en transacciones listas para analizarse.\n",
    "\n",
    "Optimización posible: Si los datos son muy grandes, considera filtrar productos o pedidos que ocurran muy poco, ya que tienen menor impacto en las reglas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder  # Herramienta para transformar transacciones en formato adecuado para Apriori\n",
    "\n",
    "# Codificar las transacciones en una matriz binaria (cada fila representa una transacción)\n",
    "te = TransactionEncoder()\n",
    "te_array = te.fit(transactions).transform(transactions)  # Codificación binaria\n",
    "dfBinary = pd.DataFrame(te_array, columns=te.columns_)  # Convertir la matriz en un DataFrame de pandas\n",
    "\n",
    "# Validar la matriz binaria generada\n",
    "print(f\"Matriz binaria generada (primeras filas):\\n{dfBinary.head()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comentario del bloque:\n",
    "Propósito: Transforma las transacciones en una representación binaria adecuada para el algoritmo Apriori.\n",
    "\n",
    "Optimización posible: Si se tienen muchas columnas, podría emplearse una matriz dispersa (scipy.sparse) desde el inicio, lo cual ahorra memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori  # Importar función Apriori\n",
    "\n",
    "# Calcular conjuntos frecuentes con un soporte mínimo establecido\n",
    "min_support = 0.01  # Este umbral determina la frecuencia mínima de los ítems\n",
    "frequent_itemsets = apriori(dfBinary, min_support=min_support, use_colnames=True)\n",
    "\n",
    "# Verificar los conjuntos frecuentes generados\n",
    "print(f\"Conjuntos frecuentes encontrados:\\n{frequent_itemsets.head()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comentario del bloque:\n",
    "Propósito: Genera los conjuntos de ítems frecuentes según el umbral de soporte.\n",
    "\n",
    "Optimización posible: Si la matriz binaria es muy grande, calcular estos conjuntos en bloques o usar estructuras dispersas puede mejorar el rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import association_rules  # Importar función para reglas de asociación\n",
    "\n",
    "# Generar reglas de asociación a partir de los conjuntos frecuentes\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.3)\n",
    "\n",
    "# Mostrar un subconjunto de las reglas generadas\n",
    "print(f\"Reglas de asociación generadas (primeras filas):\\n{rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comentario del bloque:\n",
    "Propósito: Extrae reglas de asociación usando la métrica de confianza mínima especificada.\n",
    "\n",
    "Optimización posible: Filtrar conjuntos frecuentes antes de aplicar reglas puede reducir el cálculo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar reglas por el indicador de lift (valor mínimo de 1.0)\n",
    "filtered_rules = rules[rules['lift'] >= 1.0]  # Seleccionar solo reglas con buen impacto\n",
    "print(f\"\\nReglas filtradas (lift >= 1.0):\\n{filtered_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comentario del bloque:\n",
    "Propósito: Filtra las reglas generadas para centrarse en las más útiles (lift >= 1 indica una relación positiva significativa).\n",
    "\n",
    "Optimización posible: Implementar un filtrado más específico (por confianza, soporte, etc.) para reducir reglas irrelevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix  # Importar matriz dispersa para optimización\n",
    "\n",
    "# Convertir la matriz binaria a una matriz dispersa\n",
    "order_matrix_sparse = csr_matrix(dfBinary.values)\n",
    "print(f\"Tamaño de la matriz dispersa: {order_matrix_sparse.shape}\")\n",
    "\n",
    "# Calcular soporte manualmente para comparar resultados\n",
    "import numpy as np\n",
    "item_support = np.array(order_matrix_sparse.sum(axis=0)).flatten() / order_matrix_sparse.shape[0]\n",
    "print(f\"Ítems con soporte (manual):\\n{item_support[:10]}\")  # Mostrar soporte de los primeros 10 ítems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comentario del bloque:\n",
    "Propósito: Usa matrices dispersas para ahorrar memoria y agilizar los cálculos.\n",
    "\n",
    "Optimización: Aquí ya se implementa una mejora significativa al usar estructuras eficientes como csr_matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # Para medir rendimiento\n",
    "\n",
    "# Medir tiempo al usar la implementación original (mlxtend.apriori)\n",
    "start_time = time.time()\n",
    "frequent_itemsets_apriori = apriori(dfBinary, min_support=0.02, use_colnames=True)\n",
    "mlxtend_time = time.time() - start_time\n",
    "\n",
    "# Medir tiempo al usar la matriz dispersa\n",
    "start_time = time.time()\n",
    "# Aquí se podría implementar Apriori optimizado con matriz dispersa\n",
    "sparse_time = time.time() - start_time\n",
    "\n",
    "print(f\"Tiempo con mlxtend.apriori: {mlxtend_time:.2f} segundos\")\n",
    "print(f\"Tiempo con matriz dispersa: {sparse_time:.2f} segundos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comentario del bloque:\n",
    "Propósito: Compara el tiempo de ejecución entre la implementación estándar y una optimizada.\n",
    "\n",
    "Optimización posible: Implementar un Apriori modificado que aproveche la matriz dispersa para cálculos más rápidos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sugerencias adicionales para optimizar:\n",
    "Pre-filtrar datos: Reducir la cantidad de ítems y transacciones desde el inicio, eliminando ítems poco frecuentes.\n",
    "\n",
    "Implementar Apriori manualmente: Aprovechando matrices dispersas y paralelización.\n",
    "\n",
    "Analizar métricas: Ajustar min_support y min_threshold para balancear calidad de resultados y tiempo de cálculo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VERSION DE CODIGO 2 (MEJORADA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frecuentes calculados manualmente: 120 conjuntos.\n",
      "Tiempo total: 112.79 segundos.\n",
      "Reglas generadas: 3\n",
      "Regla: {'antecedent': [23736], 'consequent': [2641], 'support': np.float64(0.018443856747631642), 'confidence': np.float64(0.3318250377073907)}\n",
      "Regla: {'antecedent': [24656], 'consequent': [2641], 'support': np.float64(0.013566142566439803), 'confidence': np.float64(0.32095203750450774)}\n",
      "Regla: {'antecedent': [24656], 'consequent': [25128], 'support': np.float64(0.012727785441547455), 'confidence': np.float64(0.30111792282726285)}\n"
     ]
    }
   ],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "\n",
    "# Cargar los datos desde URLs públicas\n",
    "url_products = \"https://github.com/it-ces/Rules-puj/blob/main/products.csv?raw=true\"\n",
    "url_orders = \"https://github.com/it-ces/Rules-puj/blob/main/order_products__train.csv?raw=true\"\n",
    "\n",
    "products = pd.read_csv(url_products)  # Cargar productos\n",
    "orders = pd.read_csv(url_orders)  # Cargar pedidos\n",
    "\n",
    "# Realizar un merge de las tablas basándose en product_id\n",
    "dfMerged = pd.merge(orders, products, on=\"product_id\", how=\"inner\")\n",
    "\n",
    "# Agrupar productos por transacción para crear listas de transacciones\n",
    "transactions = dfMerged.groupby(\"order_id\")[\"product_name\"].apply(list).tolist()\n",
    "print(f\"Primeras transacciones:\\n{transactions[:5]}\")\n",
    "# Codificar las transacciones en una matriz binaria dispersa\n",
    "\n",
    "item_mapping = {item: idx for idx, item in enumerate(sorted(set(item for transaction in transactions for item in transaction)))}\n",
    "rows, cols = [], []\n",
    "for row_idx, transaction in enumerate(transactions):\n",
    "    for item in transaction:\n",
    "        rows.append(row_idx)\n",
    "        cols.append(item_mapping[item])\n",
    "\n",
    "# Crear la matriz dispersa\n",
    "order_matrix_sparse = csr_matrix(([1] * len(rows), (rows, cols)), shape=(len(transactions), len(item_mapping)))\n",
    "\n",
    "# Función para calcular soporte de elementos individuales\n",
    "def calculate_support(item_indices, data_matrix):\n",
    "    \"\"\" Calcula el soporte de un conjunto de ítems. \"\"\"\n",
    "    item_mask = data_matrix[:, item_indices].toarray().all(axis=1)\n",
    "    support = np.sum(item_mask) / data_matrix.shape[0]\n",
    "    return support\n",
    "\n",
    "# Función para generar ítems frecuentes utilizando matrices dispersas\n",
    "def apriori_manual(data_matrix, min_support):\n",
    "    \"\"\" Implementación del algoritmo Apriori optimizada. \"\"\"\n",
    "    num_items = data_matrix.shape[1]\n",
    "    frequent_itemsets = []\n",
    "    current_itemsets = [[i] for i in range(num_items)]\n",
    "    \n",
    "    while current_itemsets:\n",
    "        next_itemsets = []\n",
    "        item_supports = []\n",
    "        \n",
    "        # Calcular soporte para cada conjunto actual\n",
    "        for itemset in current_itemsets:\n",
    "            support = calculate_support(itemset, data_matrix)\n",
    "            if support >= min_support:\n",
    "                frequent_itemsets.append((itemset, support))\n",
    "                item_supports.append(itemset)\n",
    "        \n",
    "        # Generar nuevas combinaciones de ítems frecuentes actuales\n",
    "        for i in range(len(item_supports)):\n",
    "            for j in range(i + 1, len(item_supports)):\n",
    "                combined_itemset = sorted(set(item_supports[i]) | set(item_supports[j]))\n",
    "                if len(combined_itemset) == len(item_supports[i]) + 1:\n",
    "                    next_itemsets.append(combined_itemset)\n",
    "        \n",
    "        current_itemsets = next_itemsets  # Actualizar conjuntos actuales\n",
    "    \n",
    "    return frequent_itemsets\n",
    "\n",
    "# Ajustar los umbrales para balancear calidad y rendimiento\n",
    "min_support = 0.01  # Ajusta según la calidad de reglas requerida\n",
    "\n",
    "# Medir tiempo con mlxtend.apriori\n",
    "start_time = time.time()\n",
    "frequent_itemsets_apriori = apriori(pd.DataFrame(order_matrix_sparse.toarray(), columns=item_mapping.keys()), \n",
    "                                    min_support=min_support, use_colnames=True)\n",
    "mlxtend_time = time.time() - start_time\n",
    "\n",
    "# Medir tiempo con Apriori manual optimizado\n",
    "start_time = time.time()\n",
    "frequent_itemsets_manual = apriori_manual(order_matrix_sparse, min_support)\n",
    "sparse_time = time.time() - start_time\n",
    "\n",
    "print(f\"Frecuentes calculados manualmente: {len(frequent_itemsets_manual)} conjuntos.\")\n",
    "print(f\"Tiempo con mlxtend.apriori: {mlxtend_time:.2f} segundos\")\n",
    "print(f\"Tiempo con matriz dispersa (manual): {sparse_time:.2f} segundos.\")\n",
    "\n",
    "# Analizar métricas de las reglas\n",
    "def generate_rules(frequent_itemsets, min_confidence):\n",
    "    \"\"\" Generar reglas de asociación. \"\"\"\n",
    "    rules = []\n",
    "    for itemset, support in frequent_itemsets:\n",
    "        if len(itemset) > 1:\n",
    "            for i in range(len(itemset)):\n",
    "                antecedent = itemset[:i] + itemset[i+1:]\n",
    "                consequent = [itemset[i]]\n",
    "                antecedent_support = calculate_support(antecedent, order_matrix_sparse)\n",
    "                \n",
    "                # Calcular confianza y lift\n",
    "                if antecedent_support > 0:\n",
    "                    confidence = support / antecedent_support\n",
    "                    if confidence >= min_confidence:\n",
    "                        rules.append({\n",
    "                            'antecedent': antecedent,\n",
    "                            'consequent': consequent,\n",
    "                            'support': support,\n",
    "                            'confidence': confidence\n",
    "                        })\n",
    "    return rules\n",
    "\n",
    "min_confidence = 0.3\n",
    "rules = generate_rules(frequent_itemsets_manual, min_confidence)\n",
    "print(f\"Reglas generadas: {len(rules)}\")\n",
    "for rule in rules[:5]:  # Mostrar las primeras reglas\n",
    "    print(f\"Regla: {rule}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicación detallada del código:\n",
    "Matrices dispersas:\n",
    "\n",
    "Optimizamos el uso de memoria representando transacciones en una matriz dispersa.\n",
    "\n",
    "csr_matrix es eficiente en operaciones como combinaciones y cálculos booleanos.\n",
    "\n",
    "Implementación manual de Apriori:\n",
    "\n",
    "Generamos ítems frecuentes iterativamente filtrando por el soporte mínimo.\n",
    "\n",
    "Se aprovechan las propiedades de las matrices dispersas para calcular el soporte de conjuntos grandes rápidamente.\n",
    "\n",
    "Paralelización:\n",
    "\n",
    "Aunque no se ha paralelizado en este código, librerías como multiprocessing o joblib pueden dividir la carga del cálculo entre múltiples núcleos de CPU.\n",
    "\n",
    "Métricas ajustables:\n",
    "\n",
    "min_support y min_confidence son parámetros clave para ajustar la cantidad y calidad de las reglas generadas, dependiendo de los datos y del uso final."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
